# ⚙️Gearbox fault detection using machine learning 
<div style = 'font-family:serif;font-size:16px'> <h3 align = 'center'>Abstract</h3> <p> In this notebook i have worked on gearbox fault diagnosis prediction.The problem is a binary classification, with 2 classes of broken(label 0) and healthy(label 1). Sensor sampling rate is 30hz so we gather 30 samples per second. data shape after concatinating broken with healthy is 2021119 samples with 5 inputs. From the KDE plots we have found that adding a threshold for a1 sensor as a new feature maybe helpful but then from the base model feature importance we found that it's better to drop it to increase the speed of gridsearch. For numerosity reduction we           windowed each 90 samples and extract median and standard deviation from these windows(i.e we have turned each               3 seconds of data,in other words each 90 samples, to 1 single observation). From the decision tree feature importance we found that dropping some           features may speed up next steps of ensemble modeling. Then we used random foreest and stochastic gradient boosting for improving the base model results. </p> <h3  align = 'center'>Methods</h3> <p> First we extract some statistic features(median and standard deviation) from windows of data, each window contained         90 samples, and turned them to single observations. Then for the base line model we have used single decision tree. The main goal of using this model was           finding feature importances to drop features with 0 importance(i.e features that were not used in the base tree).           After this section we realized that we could drop a1_thr_mode and load_std features with losing a small amount of           information but speeding up the gridsearch. Then untill this section we had 22456 samples with 9 input features. Using gridsearch with cross validation is really time consuming in this scenario, so we build up a custom cv               in this approach the model is validated on a vaildation set instead of cross validation[section 7-3-2]. So we used         this with gridsearch then tuned the hyperparameters of random forest with respect of the accuracy on the validation         set. From the confusion matrix we realized that in this system predicting broken state as healthy is more dangerous         so we have used the precision score as another evaluation metric. For the gradient boosting hyper parameter tuning         we have used the same approach, after gridsearch we found that instead of batch learning using stochastic approach         with sub_sample rate of 0.8 is more accurate, so the gradient boosting name has changed to stochastic gradient boosting. </p> <h3  align = 'center'>Results</h3> <p> At the end in section 8 we have used a plot, accuracy on the final test set as x axis and precision on the test set as y axis. Then we ploted each model         accuracy and precision on this figure. we conclude that stochastic gradient boosting is the best among others with         respect of accuracy and precision on the test set. </p> </div>
